# -*- coding: utf-8 -*-
"""ML_Project_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fXeBDSC-1JmrhEo_LbxWTXYMd6Nz_mK4
"""

########## Install scikit-learn ##########

# !pip uninstall scikit-learn -y
# !pip install Cython
# !pip install git+git://github.com/scikit-learn/scikit-learn.git

########## Install Microsoft Light GBM ##########

# !pip install lightgbm

########## Install XGBoost ##########

# !pip install xgboost

########## Import libraries ##########

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import sklearn.metrics as sklm
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
import xgboost as xgb
import lightgbm as lgb

########## Scaling of data ##########

def scaledData(data):
    scaler = StandardScaler()
    scaler.fit(data)
    scaledData = scaler.transform(data)
    return scaledData

########## Finding Principal Components of X_train ##########

def compute_PCA(X):
  pca = PCA(n_components=50)
  X_pca = pca.fit_transform(X)
  return X_pca

########## Load csv and prepara the datasets ########## 

def load(csv_path):
    df = pd.read_csv(csv_path,dtype={'fullVisitorId': 'str'})
    labelColumn = "totals_transactionRevenue"
    y_data = df[labelColumn]
    X_data = df.drop(['fullVisitorId', labelColumn],axis = 1)
    y = np.log1p(y_data.values)
    X = scaledData(X_data)
    #X = compute_PCA(X)      # Un-comment for PCA 
    return df, X, y

########## Load Whole Training Data ##########

train_df, X, y = load("finalEncodedData.csv") 

########## Load 10% Training Sample data for faster training ########## 
########## Run only once if comparing different algorithms and configurations ##########

# train_df = train_df.sample(180000)

########## Gradient Boosting Regressor  ##########

#-----#-----#----- With Default Parameters #-----#-----#-----

estimator = GradientBoostingRegressor()      

#-----#-----#-----For Parameter tuning #-----#-----#-----

# tuned_parameters = [ { 'loss': ['ls', 'lad'], 'learning_rate': [0.01,0.1], 'n_estimators':[100,300], 'max_depth': [3,5]} ] 

#-----#-----#----- Custom Parameter Run #-----#-----#-----

# estimator = GradientBoostingRegressor(n_estimators=1000,learning_rate=0.01, max_depth=5)

########## Random Forest Regressor ##########

#-----#-----#----- With Default Parameters #-----#-----#-----

# estimator = RandomForestRegressor()

#-----#-----#-----For Parameter tuning #-----#-----#-----

# tuned_parameters = [ {'max_features': ["auto", "sqrt"], 'warm_start': [True, False], 'n_estimators':[400,800], 'max_depth': [10,20]} ]

#-----#-----#----- Custom Parameter Run #-----#-----#-----

# estimator = RandomForestRegressor(n_estimators=1000, max_depth=20)

########## AdaBoost Regressor ##########

#-----#-----#----- With Default Parameters #-----#-----#-----

# estimator = AdaBoostRegressor()

#-----#-----#----- For Parameter tuning #-----#-----#-----

# tuned_parameters = [ {'loss':['linear', 'square'], 'learning_rate': [0.01, 0.1, 1.0], 'n_estimators':[100,300], 'random_state': [None,42]} ]

#-----#-----#----- Custom Parameter Run #-----#-----#-----

# estimator = AdaBoostRegressor(learning_rate = 1.0, n_estimators = 50)

########## Microsoft Light GBM Regressor ##########

#-----#-----#----- With Default Parameters #-----#-----#-----

# estimator = lgb.LGBMRegressor()

#-----#-----#-----For Cross Validation Folds #-----#-----#-----

# lgtrain = lgb.Dataset(X,y)

# lgbm_params =  { 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'n_jobs':-1, "learning_rate": 0.05,"num_leaves": 31, "max_depth": 5, "reg_alpha": 0.05, "reg_lambda": 0.1 }

# lgb_cv = lgb.cv(params = lgbm_params, train_set = lgtrain, num_boost_round = 1500, stratified=False, nfold = 5, verbose_eval = 100, seed = 42, early_stopping_rounds = 50)

# optimal_rounds = np.argmin(lgb_cv['rmse-mean'])

# best_cv_score = min(lgb_cv['rmse-mean'])

# print("optimal rounds",optimal_rounds)
# print("best_cv_score",best_cv_score)

# estimator = lgb.train(params = lgbm_params, train_set = lgtrain, num_boost_round = optimal_rounds + 1, verbose_eval = 50)

#-----#-----#----- Custom Parameter Run #-----#-----#-----

# estimator = lgb.LGBMRegressor(boosting_type='dart', num_leaves=28, max_depth=5, learning_rate=0.1, n_estimators=10000, bagging_freq = 50, feature_fraction = 0.5, reg_alpha=0.01, reg_lambda=0.01, bagging_fraction = 0.5, n_jobs = -1, metric= 'rmse')

# estimator = lgb.LGBMRegressor(boosting_type='dart', num_leaves=64, max_depth=7, learning_rate=0.001, n_estimators=10000, bagging_freq = 100, feature_fraction = 0.7, reg_alpha=0.01, reg_lambda=0.01, bagging_fraction = 0.7, n_jobs = 4, metric= 'rmse')

# estimator = lgb.LGBMRegressor(n_estimators=10000, learning_rate=0.05)

# estimator = lgb.LGBMRegressor(boosting_type='gbdt', colsample_bytree=0.8, learning_rate=0.01, max_depth=5, metric='rmse', n_estimators=10000, n_jobs=-1, num_leaves=32, reg_alpha=0.05, reg_lambda=0.05, subsample=0.8)

########## XGBoost Regressor ##########

#-----#-----#----- With Default Parameters #-----#-----#-----

# estimator = xgb.XGBRegressor()

#-----#-----#----- Custom Parameter Run #-----#-----#-----

# estimator = xgb.XGBRegressor(booster='gbtree', objective='reg:linear', eval_metric='rmse', learning_rate= 0.05, max_depth= 7, reg_alpha=0.1, reg_lambda=0.1, random_state= 42, nthread=-1, silent=True)

# estimator = xgb.XGBRegressor(booster='gbtree', objective='reg:linear', eval_metric='rmse', learning_rate= 0.01, max_depth= 10, reg_alpha=0.05, reg_lambda=0.1, random_state= 42, nthread=-1, silent=True)

estimator.fit(X, y)

########## If using parameter grid for hyper-tuning ##########

# print("Best Estimator: ",estimator.best_estimator_)

# print("Best Parameters: ",estimator.best_params_)

# print("Best CV Score: ",estimator.best_score_)

########## Load Test Data ##########

test_df, X_test, y_test = load("finalEncodedTestData.csv")

########## Prepare test label for validation ##########

fullVisitorId_test_prep = test_df[['fullVisitorId','totals_transactionRevenue']]
aggregated_df = fullVisitorId_test_prep.groupby(["fullVisitorId"]).sum()
yTest = np.log1p(aggregated_df["totals_transactionRevenue"].values)

########## Predict the transaction value ##########

prediction_test = estimator.predict(X_test)

########## Sum and take log of all transaction values per user ##########

prediction_test[prediction_test<0] = 0.0
prediction_inv = np.expm1(prediction_test)
fullVisitorId_test = test_df['fullVisitorId']
result_df = pd.concat([pd.DataFrame(fullVisitorId_test.values,columns=["fullVisitorId"]),pd.DataFrame(prediction_inv,columns=["totals_transactionRevenue"])],axis=1)
aggregated_df_test = result_df.groupby(["fullVisitorId"]).sum()
totals_transactionRevenue = np.log1p(aggregated_df_test["totals_transactionRevenue"])
final_df = pd.DataFrame(totals_transactionRevenue)
pred_final = final_df["totals_transactionRevenue"].values

########## Evaluation based on metrics ##########

print("R2: ",sklm.r2_score(yTest, pred_final)) 
print("explained_variance_score: ",sklm.explained_variance_score(yTest, pred_final)) 
print("mean_absolute_error: ",sklm.mean_absolute_error(yTest, pred_final)) 
print("mean_squared_error: ",sklm.mean_squared_error(yTest, pred_final)) 
print("median_absolute_error: ",sklm.median_absolute_error(yTest, pred_final)) 
print("root_mean_squared_error: ",np.sqrt(sklm.mean_squared_error(yTest, pred_final)))

